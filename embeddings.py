''''
1. https://github.com/royranadeep/LLM-with-RAG-using-MedQA-Pubmed-wiki-/blob/main/Llama3_RAG_FAISS_Abstract.ipynb
2. https://github.com/cristianleoo/rag-knowledge-graph/blob/main/faiss/demo.ipynb

These two repositieries has implemeted the concept of doing embeddings and then creating a vector database. I took understanding from
1. repo and then utilised the 2. with the help of Chatgpt for my usage 

https://github.com/facebookresearch/faiss/wiki/Faiss-indexes
'''
import pandas as pd
import numpy as np
import json
from tqdm.auto import tqdm
import faiss
from sentence_transformers import SentenceTransformer
import os
import pickle

def load_chunks(file_path):
    """Load the processed dataframe with chunks"""
    df = pd.read_csv(file_path)
    print(f"Loaded dataframe with {len(df)} rows")
    return df

# this was implemented with help of 2. repo
def create_embeddings(df, model_name="all-MiniLM-L6-v2"):
    """Create embeddings for all chunks in the dataframe"""
    # Initialize the embedding model
    print(f"Loading embedding model: {model_name}")
    embedder = SentenceTransformer(model_name)
    
    # Prepare lists to store data
    all_chunks = []
    chunk_to_row_map = []  # Maps chunk index to original dataframe row
    
    # Extract all chunks from the dataframe
    print("Extracting chunks from dataframe...")
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        try:
            chunks = json.loads(row['wiki_chunks'])
            for chunk_idx, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                # Store mapping from chunk to original row with ALL relevant information
                chunk_to_row_map.append({
                    'df_index': idx,
                    'chunk_index': chunk_idx,
                    'question': row['question'],
                    'options': row.get('options'),
                    'correct_index': row.get('correct_index', row.get('answer_idx')),
                    'category': row.get('category'),
                })
        except:
            print(f"Error processing row {idx}")
            continue
    
    print(f"Total chunks extracted: {len(all_chunks)}")
    
    # Generate embeddings for all chunks
    print("Generating embeddings...")
    chunk_embeddings = embedder.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True)
    
    print(f"Embeddings shape: {chunk_embeddings.shape}")
    
    return chunk_embeddings, all_chunks, chunk_to_row_map



# this function was fully generated by ChatGPT.
def build_faiss_index(embeddings, index_path="faiss_index.bin"):
    """Build a FAISS index from the embeddings"""
    dimension = embeddings.shape[1]  # Embedding dimension
    num_vectors = embeddings.shape[0]  # Number of chunks
    
    # Configure FAISS index parameters
    nlist = min(8, num_vectors // 10)  # Number of Voronoi cells (inverted lists)
    if nlist < 1:
        nlist = 1
    M = 2  # Number of sub-quantizers for PQ
    nbits = 8  # Bits per sub-quantizer
    
    # Create the index factory string for IVFPQ
    index_factory_string = f"IVF{nlist},PQ{M}x{nbits}"
    
    print(f"Creating FAISS index with: {index_factory_string}")
    
    # Instantiate the FAISS index
    index = faiss.index_factory(dimension, index_factory_string)
    index.nprobe = min(8, nlist)  # Number of cells to probe during search
    
    print("Training index...")
    index.train(embeddings)  # Train on the embeddings
    
    print("Adding vectors to index...")
    index.add(embeddings)  # Add the embeddings to the index
    
    print(f"Index is trained: {index.is_trained}")
    print(f"Number of vectors in index: {index.ntotal}")
    
    # saving the faisss in the form vector database that can be loaded and used later usage, so that we don't need to regenerate the embeddings each time
    print(f"Saving index to {index_path}")
    faiss.write_index(index, index_path)
    
    return index

def save_metadata(all_chunks, chunk_to_row_map, output_dir="vector_db"):
    """Save metadata for the vector database"""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Save chunks, the wikipedia chunks
    with open(os.path.join(output_dir, "chunks.pkl"), "wb") as f:
        pickle.dump(all_chunks, f)
    
    # Save mapping, the simple mapping of chunk index to original row
    with open(os.path.join(output_dir, "chunk_map.pkl"), "wb") as f:
        pickle.dump(chunk_to_row_map, f)
    
    print(f"Metadata saved to {output_dir}")

if __name__ == "__main__":
    # Load the processed data with chunks
    df = load_chunks('/home/6082/Ein/chunked_wikipedia_data.csv')
    
    # Create embeddings for all chunks
    embeddings, chunks, chunk_map = create_embeddings(df)
    
    # Build FAISS index
    index = build_faiss_index(embeddings)
    
    # Save metadata
    save_metadata(chunks, chunk_map)
    
    print("Vector database creation complete!")
    
    # Optional: Test a simple query
    print("\nTesting a simple query...")
    
    # Load the model again for the query
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    
    # Create a test query from the first question in the dataframe
    test_question = df.iloc[0]['question']
    print(f"Test question: {test_question}")
    
    # Encode the query
    query_embedding = embedder.encode([test_question], convert_to_numpy=True)
    
    # Search the index
    k = 3  # Number of results to return
    distances, indices = index.search(query_embedding, k)
    
    print(f"Top {k} most similar chunks:")
    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
        print(f"{i+1}. Distance: {dist:.4f}")
        print(f"   Chunk: {chunks[idx][:100]}...")
        print(f"   From question: {chunk_map[idx]['question'][:100]}...")
        print()